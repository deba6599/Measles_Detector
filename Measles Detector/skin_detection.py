# -*- coding: utf-8 -*-
"""Skin_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P0YTdpukLa3rcY7l61A5Y1iiQOrN51au
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import torchvision
import torchvision.transforms as transforms
import time



def get_data(train_folder, test_folder):
    
    transform = transforms.Compose([transforms.Resize((300,300)), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

    trainset = torchvision.datasets.ImageFolder(root = train_folder, transform = transform)
    trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)
    
    testset = torchvision.datasets.ImageFolder(root = test_folder, transform = transform)
    testloader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=True, num_workers=2)
    
    return trainloader, testloader, trainset.classes

import torch
import torch.nn as nn
import torch.nn.functional as F


class Classifier(nn.Module):
    def __init__(self, out1, out2, in_fea = 3, out_fea = 2):
        super(Classifier, self).__init__()
        self.out2 = out2
        self.pad = nn.ZeroPad2d(2)
        self.conv1 = nn.Conv2d(in_fea, out1, 5)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
        self.conv2 = nn.Conv2d(out1, out2, 5)
        self.fc1 = nn.Linear(out2*75*75, 128)
        self.fc2 = nn.Linear(128,64)
        self.fc3 = nn.Linear(64, out_fea)
        
    def forward(self, x):
        x = F.relu(self.pool(self.conv1(self.pad(x))))
        x = F.relu(self.pool(self.conv2(self.pad(x))))
        x = x.view(-1, self.out2*75*75)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

train_folder_location = "/content/drive/MyDrive/Skin_Detection/Data/train"
test_folder_location = "/content/drive/MyDrive/Skin_Detection/Data/test"

train_loader, test_loader, classes = get_data(train_folder_location, test_folder_location)

import matplotlib.pyplot as plt
import numpy as np

def imshow(img):
    img = img / 2 + 0.5     # unnormalize
    npimg = img.numpy()
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    #plt.imshow(npimg)
    plt.show()
    
dataiter = iter(train_loader)
images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))
print(' '.join('%5s' % classes[labels[j]] for j in range(8)))

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

net = Classifier(6, 16)
net = net.train()
net = net.to(device)

device

import torch.optim as optim

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

step = 0
loss_train = []
loss_val = []
min_loss = 100
patience = 8
training_loss_store = []
validation_loss_store = []

file = open('logs_test1_epoch30.txt', 'w')
print('training started.............................................')
file.write('training started.............................................\n')
start_time = time.time()

for epoch in range(30):  # loop over the dataset multiple times
    file.write('##############################TRAINING###############################\n')
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        step+=1
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)
        #inputs, labels = data

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss_train.append(loss.item())
        training_loss_store.append([epoch, loss.item()])
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 10 == 9:    # print every 2000 mini-batches
            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 100))
            file.write('epoch = '+ str(epoch + 1) + '\t' +'step = '+ str(step) +'\t'+'train_loss = '+'\t'+str(np.mean(loss_train)) +'\n')
            running_loss = 0.0
            loss_train = []
            
            
    print('Finished training for epoch ' + str(epoch) + ' time taken = ' + str(time.time() - start_time))
    file.write('Finished training for epoch ' + str(epoch) + ' time taken = ' + str(time.time() - start_time) + '\n')
    file.write('##################################evaluation##############################\n')
    print('################################evaluation###########################\n')
    with torch.no_grad():
        val_loss = 0
        net.eval()
        
        for i, data in enumerate(test_loader, 0):
            step+=1
            inputs, labels = data[0].to(device),data[1].to(device)
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            loss_val.append(loss.item())
            validation_loss_store.append([epoch, loss.item()])
            val_loss += loss
            
        val_loss = val_loss/float(i + 1)
        
        if val_loss < min_loss:
            min_loss = val_loss
            no_impr_epoch = 0
            
            #save the best model
            #torch.save(net.state_dict(), "weight/" + "epoch_" + str(epoch+1) + "loss_" + str(val_loss) + ".pt")
            torch.save(net.state_dict(), "epoch_" + str(epoch+1) + ".pt")
            
            print('performance improved with validation loss ' + str(val_loss))
            file.write('--------------------------------------------------------------------\n')
            file.write('performance improved with validation loss =  ' + str(val_loss) + '\n')
            
            file.write('epoch = '+ str(epoch + 1) + '\t' +'step = '+ str(step) +'\t'+'val_loss = '+'\t'+str(np.mean(loss_val)) +'\n')
            file.write('--------------------------------------------------------------------\n\n')
            loss_val = []
        else:
            no_impr_epoch += 1
            print('no improvement with prev best model ' + str(no_impr_epoch) + 'th')
            file.write('no improvement with prev best model ' + str(no_impr_epoch) + 'th \n')
            
        if no_impr_epoch > patience:
            print('stop training')
            file.write('stop training')
            break
        

print('Finished Training')


file.write('Finished Training................................................\n')
end_time = time.time()
file.write('Training time:- ' + str(end_time - start_time))
file.close()

#Release all unoccupied cached memory in GPU
if(torch.cuda.is_available()):
    torch.cuda.empty_cache()

PATH = "/content/epoch_8.pt"
model = Classifier(6, 16)
state_dict = torch.load(PATH, map_location = 'cuda:0')
model.load_state_dict(state_dict)
model.eval()
model.cuda()

dataiter = iter(test_loader)
images, labels = dataiter.next()

imshow(torchvision.utils.make_grid(images))
print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(8)))

outputs = model(images.to(device))

_, predicted = torch.max(outputs, 1)

print('Predicted: ', ' '.join('%5s' % classes[predicted[j]] for j in range(8)))

correct = 0
total = 0
with torch.no_grad():
    for data in test_loader:
        images, labels = data[0].to(device), data[1].to(device)
        outputs = net(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

print('Accuracy of the network on the complete test loader: %d %%' % (100 * correct / total))

class_correct = list(0. for i in range(2))
class_total = list(0. for i in range(2))

with torch.no_grad():
    for data in test_loader:
        images, labels = data[0].cuda(), data[1].cuda()
        outputs = net(images)
        _, predicted = torch.max(outputs, 1)
        c = (predicted == labels).squeeze() #This part does'nt work in case of GPU's
        c = (predicted == labels)
        for i in range(8):
            label = labels[i].item()
            class_correct[label] += c[i].item()
            class_total[label] += 1

for i in range(2):
    print('Accuracy of %5s : %2d %%' % (
        classes[i], 100 * class_correct[i] / class_total[i]))

